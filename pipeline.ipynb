{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Notebook\n",
    "\n",
    "This notebook aims to represent the DeepLabCut output analysis, using example data. \n",
    "So we begin by importing our packages, called for now \"dlc\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data\n",
    "As an example, we will first work on a single file. Later, we'll try batch processing. We define the path where our file is, and with the function\n",
    "`dlc.load_data.read_data` we will load our `.h5` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path =  '/Users/annateruel/ca2+img-anna-2023-01-30/videos'\n",
    "data = dlc.load_data.read_data( ,'/Users/annateruel/ca2+img-anna-2023-01-30/videos/0_neutro2DLC_resnet50_ca2+imgJan30shuffle1_500000.h5' 5, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since on this data file we have performed a tracking on the cage, we will define a list of bodyparts we want to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyparts = ['nose', 'rightear', 'leftear', 'head', 'sp1', 'sp2', 'tail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_hdf('/Volumes/ANNA_HD/ANALYSIS/EXPERIMENTS/2024/22-09-sr/analysis_tracked_roi/dlc/AD21-087-HAB2_bwDLC_resnet50_conductaJan11shuffle1_500000_filtered.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get centroid from a list of bodyparts\n",
    "\n",
    "One of the functions we can perform from the `data.py` file is to calculate the centroid of specified bodyparts in each dataframe. We can add this centroid coordinates (x,y) to our current dataframe. Let's try that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate interpolation\n",
    "\n",
    "If the tracking of some bodyparts is not perfect, we can get the interpolation of those values with likelihood under a certain threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = data.copy() #we get a copy of the dataframe and work from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating 1020 points for lc.\n",
      "NaNs after interpolation for lc x: 0\n",
      "NaNs after interpolation for lc y: 0\n",
      "Interpolating 504 points for lr.\n",
      "NaNs after interpolation for lr x: 0\n",
      "NaNs after interpolation for lr y: 0\n",
      "Interpolating 8641 points for rc.\n",
      "NaNs after interpolation for rc x: 23\n",
      "NaNs after interpolation for rc y: 23\n",
      "Interpolating 7688 points for rr.\n",
      "NaNs after interpolation for rr x: 23\n",
      "NaNs after interpolation for rr y: 23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>scorer</th>\n",
       "      <th colspan=\"21\" halign=\"left\">DLC_resnet50_conductaJan11shuffle1_500000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bodyparts</th>\n",
       "      <th colspan=\"3\" halign=\"left\">nose</th>\n",
       "      <th colspan=\"3\" halign=\"left\">rightear</th>\n",
       "      <th colspan=\"3\" halign=\"left\">leftear</th>\n",
       "      <th>head</th>\n",
       "      <th>...</th>\n",
       "      <th>lc</th>\n",
       "      <th colspan=\"3\" halign=\"left\">lr</th>\n",
       "      <th colspan=\"3\" halign=\"left\">rc</th>\n",
       "      <th colspan=\"3\" halign=\"left\">rr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coords</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>...</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192.576813</td>\n",
       "      <td>90.042381</td>\n",
       "      <td>0.989976</td>\n",
       "      <td>201.826263</td>\n",
       "      <td>77.582146</td>\n",
       "      <td>0.997141</td>\n",
       "      <td>209.012009</td>\n",
       "      <td>86.990158</td>\n",
       "      <td>0.998314</td>\n",
       "      <td>206.462845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999050</td>\n",
       "      <td>202.102158</td>\n",
       "      <td>67.029243</td>\n",
       "      <td>0.996905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192.472772</td>\n",
       "      <td>89.957989</td>\n",
       "      <td>0.995514</td>\n",
       "      <td>201.802455</td>\n",
       "      <td>77.554642</td>\n",
       "      <td>0.997967</td>\n",
       "      <td>208.990838</td>\n",
       "      <td>86.961843</td>\n",
       "      <td>0.998370</td>\n",
       "      <td>206.449585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998461</td>\n",
       "      <td>202.094839</td>\n",
       "      <td>66.990569</td>\n",
       "      <td>0.996441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>191.727154</td>\n",
       "      <td>88.780883</td>\n",
       "      <td>0.998694</td>\n",
       "      <td>201.684684</td>\n",
       "      <td>77.226272</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>208.665101</td>\n",
       "      <td>87.170066</td>\n",
       "      <td>0.999130</td>\n",
       "      <td>206.196418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998872</td>\n",
       "      <td>202.285588</td>\n",
       "      <td>66.670506</td>\n",
       "      <td>0.996053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189.864682</td>\n",
       "      <td>86.294754</td>\n",
       "      <td>0.998703</td>\n",
       "      <td>201.453250</td>\n",
       "      <td>75.604009</td>\n",
       "      <td>0.998922</td>\n",
       "      <td>206.270972</td>\n",
       "      <td>85.950256</td>\n",
       "      <td>0.999149</td>\n",
       "      <td>206.003571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998904</td>\n",
       "      <td>202.530027</td>\n",
       "      <td>66.403379</td>\n",
       "      <td>0.996617</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189.715138</td>\n",
       "      <td>85.881709</td>\n",
       "      <td>0.999165</td>\n",
       "      <td>201.488727</td>\n",
       "      <td>75.449877</td>\n",
       "      <td>0.999673</td>\n",
       "      <td>206.168562</td>\n",
       "      <td>85.855557</td>\n",
       "      <td>0.999113</td>\n",
       "      <td>206.070615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>202.553277</td>\n",
       "      <td>66.400060</td>\n",
       "      <td>0.992115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15871</th>\n",
       "      <td>185.534588</td>\n",
       "      <td>119.485684</td>\n",
       "      <td>0.999939</td>\n",
       "      <td>177.114119</td>\n",
       "      <td>130.294038</td>\n",
       "      <td>0.999337</td>\n",
       "      <td>167.374024</td>\n",
       "      <td>119.293206</td>\n",
       "      <td>0.999396</td>\n",
       "      <td>167.856163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>203.501753</td>\n",
       "      <td>79.485422</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>275.948905</td>\n",
       "      <td>146.006132</td>\n",
       "      <td>0.992301</td>\n",
       "      <td>301.329633</td>\n",
       "      <td>148.732191</td>\n",
       "      <td>0.998251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15872</th>\n",
       "      <td>185.587940</td>\n",
       "      <td>119.425258</td>\n",
       "      <td>0.999938</td>\n",
       "      <td>177.143742</td>\n",
       "      <td>130.185756</td>\n",
       "      <td>0.999347</td>\n",
       "      <td>167.398549</td>\n",
       "      <td>119.306177</td>\n",
       "      <td>0.999397</td>\n",
       "      <td>167.888614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>203.481596</td>\n",
       "      <td>79.464383</td>\n",
       "      <td>0.999977</td>\n",
       "      <td>275.948905</td>\n",
       "      <td>146.006132</td>\n",
       "      <td>0.992301</td>\n",
       "      <td>301.329633</td>\n",
       "      <td>148.732191</td>\n",
       "      <td>0.998251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15873</th>\n",
       "      <td>185.551383</td>\n",
       "      <td>119.445118</td>\n",
       "      <td>0.999939</td>\n",
       "      <td>177.133494</td>\n",
       "      <td>130.186139</td>\n",
       "      <td>0.999363</td>\n",
       "      <td>167.378805</td>\n",
       "      <td>119.310071</td>\n",
       "      <td>0.999404</td>\n",
       "      <td>167.874367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>203.472673</td>\n",
       "      <td>79.484302</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>275.948905</td>\n",
       "      <td>146.006132</td>\n",
       "      <td>0.992301</td>\n",
       "      <td>301.329633</td>\n",
       "      <td>148.732191</td>\n",
       "      <td>0.998251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15874</th>\n",
       "      <td>185.516088</td>\n",
       "      <td>119.435682</td>\n",
       "      <td>0.999937</td>\n",
       "      <td>177.132835</td>\n",
       "      <td>130.184303</td>\n",
       "      <td>0.999337</td>\n",
       "      <td>167.305087</td>\n",
       "      <td>119.283849</td>\n",
       "      <td>0.999393</td>\n",
       "      <td>167.822864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>203.507526</td>\n",
       "      <td>79.462233</td>\n",
       "      <td>0.999977</td>\n",
       "      <td>275.948905</td>\n",
       "      <td>146.006132</td>\n",
       "      <td>0.992301</td>\n",
       "      <td>301.329633</td>\n",
       "      <td>148.732191</td>\n",
       "      <td>0.998251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15875</th>\n",
       "      <td>185.528272</td>\n",
       "      <td>119.418402</td>\n",
       "      <td>0.999938</td>\n",
       "      <td>177.128770</td>\n",
       "      <td>130.195717</td>\n",
       "      <td>0.999358</td>\n",
       "      <td>167.333438</td>\n",
       "      <td>119.307683</td>\n",
       "      <td>0.999402</td>\n",
       "      <td>167.855714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>203.506760</td>\n",
       "      <td>79.464465</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>275.948905</td>\n",
       "      <td>146.006132</td>\n",
       "      <td>0.992301</td>\n",
       "      <td>301.329633</td>\n",
       "      <td>148.732191</td>\n",
       "      <td>0.998251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15876 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "scorer    DLC_resnet50_conductaJan11shuffle1_500000                         \\\n",
       "bodyparts                                      nose                          \n",
       "coords                                            x           y likelihood   \n",
       "0                                        192.576813   90.042381   0.989976   \n",
       "1                                        192.472772   89.957989   0.995514   \n",
       "2                                        191.727154   88.780883   0.998694   \n",
       "3                                        189.864682   86.294754   0.998703   \n",
       "4                                        189.715138   85.881709   0.999165   \n",
       "...                                             ...         ...        ...   \n",
       "15871                                    185.534588  119.485684   0.999939   \n",
       "15872                                    185.587940  119.425258   0.999938   \n",
       "15873                                    185.551383  119.445118   0.999939   \n",
       "15874                                    185.516088  119.435682   0.999937   \n",
       "15875                                    185.528272  119.418402   0.999938   \n",
       "\n",
       "scorer                                                                \\\n",
       "bodyparts    rightear                            leftear               \n",
       "coords              x           y likelihood           x           y   \n",
       "0          201.826263   77.582146   0.997141  209.012009   86.990158   \n",
       "1          201.802455   77.554642   0.997967  208.990838   86.961843   \n",
       "2          201.684684   77.226272   0.998939  208.665101   87.170066   \n",
       "3          201.453250   75.604009   0.998922  206.270972   85.950256   \n",
       "4          201.488727   75.449877   0.999673  206.168562   85.855557   \n",
       "...               ...         ...        ...         ...         ...   \n",
       "15871      177.114119  130.294038   0.999337  167.374024  119.293206   \n",
       "15872      177.143742  130.185756   0.999347  167.398549  119.306177   \n",
       "15873      177.133494  130.186139   0.999363  167.378805  119.310071   \n",
       "15874      177.132835  130.184303   0.999337  167.305087  119.283849   \n",
       "15875      177.128770  130.195717   0.999358  167.333438  119.307683   \n",
       "\n",
       "scorer                            ...                                    \\\n",
       "bodyparts                   head  ...         lc          lr              \n",
       "coords    likelihood           x  ... likelihood           x          y   \n",
       "0           0.998314  206.462845  ...   0.999050  202.102158  67.029243   \n",
       "1           0.998370  206.449585  ...   0.998461  202.094839  66.990569   \n",
       "2           0.999130  206.196418  ...   0.998872  202.285588  66.670506   \n",
       "3           0.999149  206.003571  ...   0.998904  202.530027  66.403379   \n",
       "4           0.999113  206.070615  ...   0.998601  202.553277  66.400060   \n",
       "...              ...         ...  ...        ...         ...        ...   \n",
       "15871       0.999396  167.856163  ...   0.999980  203.501753  79.485422   \n",
       "15872       0.999397  167.888614  ...   0.999980  203.481596  79.464383   \n",
       "15873       0.999404  167.874367  ...   0.999980  203.472673  79.484302   \n",
       "15874       0.999393  167.822864  ...   0.999980  203.507526  79.462233   \n",
       "15875       0.999402  167.855714  ...   0.999980  203.506760  79.464465   \n",
       "\n",
       "scorer                                                               \\\n",
       "bodyparts                     rc                                 rr   \n",
       "coords    likelihood           x           y likelihood           x   \n",
       "0           0.996905         NaN         NaN        NaN         NaN   \n",
       "1           0.996441         NaN         NaN        NaN         NaN   \n",
       "2           0.996053         NaN         NaN        NaN         NaN   \n",
       "3           0.996617         NaN         NaN        NaN         NaN   \n",
       "4           0.992115         NaN         NaN        NaN         NaN   \n",
       "...              ...         ...         ...        ...         ...   \n",
       "15871       0.999978  275.948905  146.006132   0.992301  301.329633   \n",
       "15872       0.999977  275.948905  146.006132   0.992301  301.329633   \n",
       "15873       0.999976  275.948905  146.006132   0.992301  301.329633   \n",
       "15874       0.999977  275.948905  146.006132   0.992301  301.329633   \n",
       "15875       0.999976  275.948905  146.006132   0.992301  301.329633   \n",
       "\n",
       "scorer                            \n",
       "bodyparts                         \n",
       "coords              y likelihood  \n",
       "0                 NaN        NaN  \n",
       "1                 NaN        NaN  \n",
       "2                 NaN        NaN  \n",
       "3                 NaN        NaN  \n",
       "4                 NaN        NaN  \n",
       "...               ...        ...  \n",
       "15871      148.732191   0.998251  \n",
       "15872      148.732191   0.998251  \n",
       "15873      148.732191   0.998251  \n",
       "15874      148.732191   0.998251  \n",
       "15875      148.732191   0.998251  \n",
       "\n",
       "[15876 rows x 36 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolator = dlc.data.Interpolation(threshold=0.95, interpolation_method='linear')\n",
    "interpolator.get_interpolation(df = df2, bodyparts = ['lc', 'lr', 'rc', 'rr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "DLCscorer = df2.columns[0][0]\n",
    "threshold = 0.95\n",
    "likelihood_col = (DLCscorer, 'nose', 'likelihood')\n",
    "if df2[likelihood_col].lt(threshold).any():\n",
    "    mask = df2[likelihood_col] < threshold\n",
    "\n",
    "    df2.loc[mask, (DLCscorer, 'nose', 'x')] = np.nan\n",
    "    df2.loc[mask, (DLCscorer, 'nose', 'y')] = np.nan\n",
    "\n",
    "    df2[(DLCscorer, 'nose', 'x')] = df2[(DLCscorer, 'nose', 'x')].interpolate(method='linear')\n",
    "    df2[(DLCscorer, 'nose', 'y')] = df2[(DLCscorer, 'nose', 'y')].interpolate(method='linear')\n",
    "\n",
    "nose2 = df2.loc[:,(DLCscorer, 'nose', slice(None))]\n",
    "nose2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>scorer</th>\n",
       "      <th>DLC_resnet50_conductaJan11shuffle1_500000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bodyparts</th>\n",
       "      <th>rr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coords</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15871</th>\n",
       "      <td>301.329633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15872</th>\n",
       "      <td>301.329633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15873</th>\n",
       "      <td>301.329633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15874</th>\n",
       "      <td>301.329633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15875</th>\n",
       "      <td>301.329633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15876 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "scorer    DLC_resnet50_conductaJan11shuffle1_500000\n",
       "bodyparts                                        rr\n",
       "coords                                            x\n",
       "0                                               NaN\n",
       "1                                               NaN\n",
       "2                                               NaN\n",
       "3                                               NaN\n",
       "4                                               NaN\n",
       "...                                             ...\n",
       "15871                                    301.329633\n",
       "15872                                    301.329633\n",
       "15873                                    301.329633\n",
       "15874                                    301.329633\n",
       "15875                                    301.329633\n",
       "\n",
       "[15876 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc = data.loc[:, (slice(None), 'rr', slice(None))]\n",
    "lc.loc[:,(slice(None), slice(None), 'x')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "x = lc.loc[:, (slice(None), slice(None), 'x')].values\n",
    "y = lc.loc[:, (slice(None), slice(None), 'y')].values\n",
    "likelihood = lc.loc[:, (slice(None), slice(None), 'likelihood')].values.flatten()  # Flatten the array\n",
    "\n",
    "x_filled = pd.DataFrame(x).fillna(method='ffill').fillna(method='bfill').values\n",
    "y_filled = pd.DataFrame(y).fillna(method='ffill').fillna(method='bfill').values\n",
    "likelihood_filled = pd.Series(likelihood).fillna(method='ffill').fillna(method='bfill').values\n",
    "\n",
    "\n",
    "# Create segments for LineCollection\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "\n",
    "# Create a LineCollection object with segments and colormap\n",
    "colormap = plt.cm.RdYlGn\n",
    "lc = LineCollection(segments, cmap=colormap, norm=plt.Normalize(likelihood.min(), likelihood.max()), linewidth=0.5, alpha=0.5)\n",
    "lc.set_array(likelihood)\n",
    "\n",
    "ax.add_collection(lc)\n",
    "\n",
    "# Add a colorbar to the plot to show what each color represents\n",
    "cbar = plt.colorbar(lc, ax=ax)\n",
    "cbar.set_label('Likelihood Value')\n",
    "\n",
    "ax.set_xlim(x.min(), x.max())\n",
    "ax.set_ylim(y.min(), y.max())\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "def plot_data(ax, data, title):\n",
    "    x = data.loc[:, (slice(None), slice(None), 'x')].values\n",
    "    y = data.loc[:, (slice(None), slice(None), 'y')].values\n",
    "    likelihood = data.loc[:, (slice(None), slice(None), 'likelihood')].values.flatten()\n",
    "\n",
    "    # Create segments for LineCollection\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "\n",
    "    # Create a LineCollection object with segments and colormap\n",
    "    colormap = plt.cm.RdYlGn\n",
    "    lc = LineCollection(segments, cmap=colormap, norm=plt.Normalize(likelihood.min(), likelihood.max()), linewidth=0.5, alpha=0.5)\n",
    "    lc.set_array(likelihood)\n",
    "\n",
    "    ax.add_collection(lc)\n",
    "    ax.set_xlim(x.min(), x.max())\n",
    "    ax.set_ylim(y.min(), y.max())\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    return lc\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "lc1 = plot_data(ax1, nose, \"Before Interpolation\")\n",
    "lc2 = plot_data(ax2, nose2, \"After Interpolation\")\n",
    "\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "cbar = fig.colorbar(lc2, cax=cbar_ax)\n",
    "cbar.set_label('Likelihood Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "lc1 = plot_data(ax1, nose, \"Before Interpolation\")\n",
    "lc2 = plot_data(ax2, nose2, \"After Interpolation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI Drawer\n",
    "\n",
    "Testing ROI Drawer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlc.analysis.time_roi import ROIDrawer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_drawer = ROIDrawer(video_path='/Users/annateruel/Desktop/videos/7_WIN_20231010_094516_1.mp4',save_dir='/Users/annateruel/Desktop/videos/', num_rois=4)\n",
    "r = roi_drawer.draw_rois()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing time in ROI\n",
    "\n",
    "1. Define ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlc.analysis.time_roi as time_roi\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.read_hdf('/Users/annateruel/Desktop/videos/11_WIN_20231010_111425_1_roi.h5')\n",
    "roi_groups = r.groupby(['index', 'shape-type'])\n",
    "polygons, _ = time_roi.PolygonROI.extract_polygons(roi_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get time (s) in each ROI for a given dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_in_roi = time_roi.TimeinRoi(fps=15)\n",
    "for poly in polygons:\n",
    "    time_in_roi.add_roi(poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/annateruel/Desktop/videos/11_WIN_20231010_111425_1DLC_resnet50_capsaicin_malesOct20shuffle1_200000_filtered.h5'\n",
    "directory = '/Users/annateruel/Desktop/videos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = 'DLC_resnet50_capsaicin_malesOct20shuffle1_200000'\n",
    "bodypart = 'Neck'\n",
    "\n",
    "tracking_data = time_in_roi.extract_tracking_data(file_path, scorer, bodypart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spent = time_in_roi.time_in_rois(tracking_data)\n",
    "time_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spent = time_in_roi.time_in_rois_dir(directory, scorer, bodypart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = '/Users/annateruel/Desktop/videos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_polygons = {}\n",
    "for file in os.listdir(video_dir):\n",
    "    if file.endswith('roi.h5'):\n",
    "        roi_file_path = os.path.join(video_dir, file)\n",
    "        print(f\"Processing ROI file: {file}\")\n",
    "\n",
    "        try:\n",
    "            roi_data = pd.read_hdf(roi_file_path)\n",
    "            roi_groups = roi_data.groupby(['index', 'shape-type'])\n",
    "            polygons, _ = time_roi.PolygonROI.extract_polygons(roi_groups)\n",
    "            # Create the key by removing '_roi.h5' from the file name\n",
    "            key = file.replace('_roi.h5', '')\n",
    "            roi_polygons[key] = polygons  # Store polygons with the modified key\n",
    "            print(f\"Extracted polygons from {file}: {len(polygons)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = 'DLC_resnet50_capsaicin_malesOct20shuffle1_200000'\n",
    "bodypart = 'Neck'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_in_roi = time_roi.TimeinRoi(fps=15)\n",
    "results_df = time_in_roi.time_in_rois_dir(directory=video_dir, rois=roi_polygons, scorer=scorer, body_part=bodypart)\n",
    "results_df            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(video_dir, \"time_in_rois_results.csv\")\n",
    "results_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the keys in the roi_polygons dictionary\n",
    "print(\"Keys in roi_polygons dictionary:\")\n",
    "for key in roi_polygons.keys():\n",
    "    print(key)\n",
    "\n",
    "# Check if the keys match the expected format\n",
    "print(\"\\nExpected keys based on tracking data files:\")\n",
    "for filename in os.listdir(video_dir):\n",
    "    if filename.endswith('filtered.h5'):\n",
    "        expected_key = filename.replace('_filtered.h5', '')\n",
    "        print(expected_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRIALS\n",
    "\n",
    "Testing some functions here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyparts = ['nose', 'rightear', 'leftear', 'head']\n",
    "file_path = '/Users/annateruel/sr-ca2+img-anna-2023-11-17/videos/AD22-56-neutro2-1DLC_dlcrnetms5_sr-ca2+imgNov17shuffle1_500000_filtered.h5'\n",
    "title = 'Demo Plot'\n",
    "path = '/Users/annateruel/sr-ca2+img-anna-2023-11-17/videos/'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_plot = dlc.plotting.TrackingPlot(style='light')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_plot.plot_directory(path, bodyparts, title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video clustering for labelling\n",
    "\n",
    "I want to create a function that can analyze a set of videos, perform a form of dimensionality reduction or clustering on them, and then identify which videos are most distinct from each other. This process will involve comparing videos based on the similarity of their pixel values, possibly in grayscale, to group similar videos into clusters. From each cluster, you can then select one representative video for labeling in your DeepLabCut model.\n",
    "\n",
    "This is a test code to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_video(video_path, frame_count=100):\n",
    "    \"\"\"\n",
    "    This function takes a video path and extracts a fixed number of frames, converts them to grayscale, resizes, and flattens them.\n",
    "    \"\"\"    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            gray_frame = cv2.resize(gray_frame, (100, 100))  # Resize for consistency\n",
    "            frames.append(gray_frame.flatten())\n",
    "            if len(frames) == frame_count:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The function load_and_preprocess_video you have is designed to process a single video. To create the video_features array that is needed for the rest of the PCA and clustering steps, you will need to call this function on each video in your directory and then average the frames to get a single feature vector for each video.\n",
    "\n",
    "Here's how you can create a function to load and preprocess features for all videos in a directory, using your existing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_features_from_directory(video_directory, frame_count=100):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses video features for all videos in a directory.\n",
    "    \n",
    "    :param video_directory: The path to the directory containing the videos.\n",
    "    :param frame_count: The number of frames to extract from each video.\n",
    "    :return: A tuple of an array of video features and a list of video paths.\n",
    "    \"\"\"\n",
    "    video_paths = [os.path.join(video_directory, f) for f in os.listdir(video_directory) if f.endswith('.mp4')]\n",
    "    video_features = []\n",
    "\n",
    "    for video_path in video_paths:\n",
    "        # Use the provided function to load and preprocess the video\n",
    "        preprocessed_frames = load_and_preprocess_video(video_path, frame_count)\n",
    "        # Calculate the mean across all frames to get a single feature vector per video\n",
    "        video_feature = np.mean(preprocessed_frames, axis=0)\n",
    "        video_features.append(video_feature)\n",
    "\n",
    "    return np.array(video_features), video_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_directory = \"/Users/annateruel/sdt_videos/\"\n",
    "video_features, video_paths = load_video_features_from_directory(video_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of setting the number of clusters to 5 was arbitrary and provided as a starting point for the example. In practice, the optimal number of clusters depends on the specific characteristics and distribution of your data.\n",
    "\n",
    "To determine the most appropriate number of clusters, you can use several methods:\n",
    "\n",
    "1. *Elbow Method*: Plot the sum of squared distances of samples to their closest cluster center for a range of number of clusters. Look for the “elbow” where the rate of decrease sharply changes, which can be considered an indicator of the optimal number of clusters.\n",
    "2. *Silhouette Score*: Calculate the mean silhouette coefficient over all samples. This gives a perspective into the density and separation of the formed clusters. The silhouette score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "3. *Gap Statistic*: Compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The optimal k is the one that maximizes the gap statistic.\n",
    "4. *Domain Knowledge*: Sometimes the optimal number of clusters is suggested by the context of the problem or domain expertise.\n",
    "\n",
    "Determining the best number of clusters for a dataset without domain knowledge can be challenging because it often depends on the context of the data and the goal of the clustering. However, there are several statistical methods that can help you decide. I will outline two of the most commonly used methods:\n",
    "\n",
    "1. **The elbow method**: This method involves plotting the explained variance as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use. The idea is to choose a small value of k that still has a low sum of squared distances (inertia).\n",
    "\n",
    "2. **The shilhouette method**: The silhouette value measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from -1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If many points have a high value, the clustering configuration is appropriate. If many points have a low or negative value, the clustering configuration may have too many or too few clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate the Sum of Squared Distances (SSD) for different values of k\n",
    "def calculate_ssd_for_k(reduced_features, k_range):\n",
    "    ssd = []\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42).fit(reduced_features)\n",
    "        ssd.append(kmeans.inertia_)\n",
    "    return ssd\n",
    "\n",
    "# Function to calculate silhouette scores for different values of k\n",
    "def calculate_silhouette_for_k(reduced_features, k_range):\n",
    "    silhouette_scores = []\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42).fit(reduced_features)\n",
    "        score = silhouette_score(reduced_features, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "    return silhouette_scores\n",
    "\n",
    "# Define the range of k you want to test\n",
    "k_range = range(2, 11)\n",
    "\n",
    "# Calculate SSD and silhouette scores\n",
    "ssd = calculate_ssd_for_k(reduced_features, k_range)\n",
    "silhouette_scores = calculate_silhouette_for_k(reduced_features, k_range)\n",
    "\n",
    "# Plotting the Elbow Method\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, ssd, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum of squared distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "\n",
    "# Plotting the Silhouette Method\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, silhouette_scores, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('Silhouette Method For Optimal k')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px \n",
    "\n",
    "def main(video_directory, n_clusters=15):\n",
    "    video_paths = [os.path.join(video_directory, f) for f in os.listdir(video_directory) if f.endswith('.mp4')]\n",
    "    video_features = []\n",
    "\n",
    "    for video_path in video_paths:\n",
    "        features = load_and_preprocess_video(video_path)\n",
    "        video_features.append(np.mean(features, axis=0))  # Mean of frames as the video feature\n",
    "\n",
    "    video_features = np.array(video_features)\n",
    "\n",
    "    # Use 3 PCA components for 3D visualization\n",
    "    pca = PCA(n_components=3)\n",
    "    reduced_features = pca.fit_transform(video_features)\n",
    "\n",
    "    # KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(reduced_features)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Create a 3D scatter plot using Plotly\n",
    "    fig = px.scatter_3d(\n",
    "        reduced_features, x=0, y=1, z=2,\n",
    "        color=labels,\n",
    "        labels={'0': 'PCA Component 1', '1': 'PCA Component 2', '2': 'PCA Component 3'},\n",
    "        title='PCA Clustering of Videos'\n",
    "    )\n",
    "\n",
    "    # Customize the plotly figure to display video names on hover\n",
    "    hover_texts = [os.path.basename(video_path) for video_path in video_paths]\n",
    "    fig.update_traces(marker_size=8, hoverinfo='text', text=hover_texts)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Select one video from each cluster\n",
    "    representative_videos = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_indices = np.where(labels == i)[0]\n",
    "        representative_videos.append(video_paths[cluster_indices[0]])  # Select the first video of each cluster for simplicity\n",
    "\n",
    "    print(\"Representative Videos:\", representative_videos)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"/Users/annateruel/sdt_videos/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying lr and loss plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/annateruel/Desktop/learning_stats2.csv' \n",
    "df = pd.read_csv(data_path, sep=';', decimal=',', engine='python')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot the loss on the primary y-axis\n",
    "ax1.plot(df['iterations'], df['loss'], color='orange', label='Loss')\n",
    "ax1.set_xlabel('Iterations')\n",
    "ax1.set_ylabel('Loss', color='orange')\n",
    "ax1.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "# Create a second y-axis for the learning rate\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df['iterations'], df['lr'], color='purple', label='Learning Rate')\n",
    "ax2.set_ylabel('Learning Rate', color='purple')\n",
    "ax2.tick_params(axis='y', labelcolor='purple')\n",
    "\n",
    "# Show the legend\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Loss and Learning Rate Through Number of Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_hdf('/Users/annateruel/Downloads/CollectedData_Paola.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

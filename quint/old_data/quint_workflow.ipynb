{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting and combining QUINT output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We collect all files of interest from each animal, for abeta intra, abeta extra, and tau, in one single folder. If all files are in the same folder it will be easier to combine. \n",
    "\n",
    "(PER COLOCAR TOTS ELS ARXIUS DE ABETA I TAU EN UNA ÚNICA CARPETA, DE TOTS ELS ANIMALS PERQUÈ SIGA MÉS FÀCIL/CÒMODE PER ACCEDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "main = '/Volumes/ANNA_HD/ANALYSIS/QUINT/abeta-Tau/data/data_B6129'\n",
    "output = '/Volumes/ANNA_HD/ANALYSIS/QUINT/abeta-Tau/output'\n",
    "animal_data = {}\n",
    "\n",
    "for animal_folder in os.listdir(main):\n",
    "    animal_path = os.path.join(main, animal_folder)\n",
    "    \n",
    "    if os.path.isdir(animal_path):\n",
    "        for marker_folder in [\"abeta_extra\",\"abeta_intra\",\"tau\"]:\n",
    "            marker_path = os.path.join(animal_path, \"nutil\", marker_folder, \"Reports\", animal_folder + \"_RefAtlasRegions\")\n",
    "            \n",
    "            # Check if the directory exists before trying to list its files\n",
    "            if os.path.isdir(marker_path):\n",
    "                csv_files = [f for f in os.listdir(marker_path) if f.endswith(\"RefAtlasRegions.csv\")]\n",
    "                        \n",
    "                for csv_file in csv_files:\n",
    "                    csv_path = os.path.join(marker_path, csv_file)\n",
    "                    if csv_file.endswith(\".csv\"):\n",
    "                        df = pd.read_csv(csv_path, sep=\";\")\n",
    "                                \n",
    "                        # Perform operations on the DataFrame\n",
    "                        result_df = df.loc[:, ('Region name', 'Object count')]\n",
    "                                \n",
    "                        # Get information about the animal and marker\n",
    "                        animal_marker_info = f\"{animal_folder}-{marker_folder}\"\n",
    "                                \n",
    "                        if marker_folder == \"abeta_extra\": \n",
    "                            output = '/Volumes/ANNA_HD/ANALYSIS/QUINT/abeta-Tau/output/abeta_extra'  \n",
    "                            # Create the output file path\n",
    "                            output_file_path = os.path.join(output, f\"{animal_marker_info}.csv\")    \n",
    "                            result_df.to_csv(output_file_path, encoding = 'utf8')\n",
    "                        elif marker_folder == \"abeta_intra\": \n",
    "                            output = '/Volumes/ANNA_HD/ANALYSIS/QUINT/abeta-Tau/output/abeta_intra'  \n",
    "                            # Create the output file path\n",
    "                            output_file_path = os.path.join(output, f\"{animal_marker_info}.csv\")    \n",
    "                            result_df.to_csv(output_file_path,  encoding = 'utf8')\n",
    "                        elif  marker_folder == \"tau\": \n",
    "                            output = '/Volumes/ANNA_HD/ANALYSIS/QUINT/abeta-Tau/output/tau'  \n",
    "                            # Create the output file path\n",
    "                            output_file_path = os.path.join(output, f\"{animal_marker_info}.csv\")    \n",
    "                            result_df.to_csv(output_file_path,  encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "directory = '/Volumes/ANNA_HD/ANALYSIS/QUINT/abeta-Tau/output/tau/'\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "    if re.match(r'^AD\\d{5}-1-.*$', file_name):\n",
    "        new_file_name = re.sub(r'-1-', r'-', file_name)\n",
    "        os.rename(os.path.join(directory, file_name), os.path.join(directory, new_file_name))\n",
    "        print(f\"Renamed: {file_name} to {new_file_name}\")\n",
    "\n",
    "print(\"Renaming completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "directory = '/Volumes/ANNA_HD/ANALYSIS/QUINT/abeta-Tau/output/tau/'\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "    if re.match(r'^AD22.*-S\\d+.*$', file_name):\n",
    "        new_file_name = re.sub(r'-S(\\d+)', r'-\\1', file_name)\n",
    "        os.rename(os.path.join(directory, file_name), os.path.join(directory, new_file_name))\n",
    "        print(f\"Renamed: {file_name} to {new_file_name}\")\n",
    "    elif re.match(r'^AD20.*-S\\d+.*$', file_name):\n",
    "        new_file_name = re.sub(r'-S(\\d+)', r'-\\1', file_name)\n",
    "        os.rename(os.path.join(directory, file_name), os.path.join(directory, new_file_name))\n",
    "        print(f\"Renamed: {file_name} to {new_file_name}\")\n",
    "\n",
    "print(\"Renaming completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Combine csv from each animal, for abeta intra, abeta extra and protein tau, using pandas add function\n",
    "\n",
    "(COMBINAR ELS DF DE CADA ANIMAL EN UN ÚNIC, SUMAR ELS COMPONENTS PER ÀREA PER AL MATEIX ANIMAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_directory = '/Volumes/ANNA_HD/ANALYSIS/QUINT/abeta-Tau/output/tau/'\n",
    "output_directory = '/Volumes/ANNA_HD/ANALYSIS/QUINT/abeta-Tau/output/tau/'\n",
    "\n",
    "group_data = {}\n",
    "\n",
    "for file_name in os.listdir(input_directory):\n",
    "    file_path = os.path.join(input_directory, file_name)\n",
    "    if file_name.startswith(\"._\"):\n",
    "        file_path = os.path.join(input_directory, file_name)\n",
    "        os.remove(file_path)\n",
    "    else:\n",
    "        prefix = file_name.split(\"-\")[0]\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if prefix not in group_data:\n",
    "            group_data[prefix] = df.set_index('Region name')\n",
    "        else:\n",
    "            group_data[prefix] = group_data[prefix].add(df.set_index('Region name'), fill_value=0)\n",
    "\n",
    "for prefix, df in group_data.items():\n",
    "    output_file_path = os.path.join(output_directory, f\"{prefix}_combined.csv\")\n",
    "    df.to_csv(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Combine information of all brain regions together. Because QUINT is showing the output of each brain region and cortical layer, the dataframe is too big and not representative. So we group brain regions by a common root. For example, all subregions of entorhinal cortex together. To do so, we load all combined.csv files we have generated and transform them again. \n",
    "\n",
    "(COMBINAR ÁREAS CEREBRALES PARA REDUCIR UN POCO EL TAMAÑO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = '/Volumes/ANNA_HD/ANALYSIS/QUINT/abeta-Tau/output/tau/'\n",
    "output_directory = '/Volumes/ANNA_HD/ANALYSIS/QUINT/abeta-Tau/output/tau/'\n",
    "\n",
    "for file_name in os.listdir(input_directory):\n",
    "    file_path = os.path.join(input_directory, file_name)\n",
    "    if file_name.startswith(\"._\"):\n",
    "        file_path = os.path.join(input_directory, file_name)\n",
    "        os.remove(file_path)\n",
    "    elif file_name.endswith(\"_combined.csv\"):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Common label'] = df['Region name'].str.split(',').str[0].str.strip()\n",
    "        grouped_df = df.groupby('Common label', as_index=False)['Object count'].sum()\n",
    "        output_file_path = os.path.join(output_directory, file_name)\n",
    "        grouped_df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Get the transpose of each dataframe and then merge all dataframes by row. Thus each row belongs to one animal.  \n",
    "\n",
    "(TRANSPUESTA DE DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "c_path = '/Volumes/ANNA_HD/ANALYSIS/QUINT/abeta-Tau/output/tau/'\n",
    "c_output = '/Volumes/ANNA_HD/ANALYSIS/QUINT/abeta-Tau/output/tau/'\n",
    "\n",
    "transposed_dataframes = {}\n",
    "\n",
    "for file_name in os.listdir(c_path):\n",
    "    if file_name.startswith(\"._\"):\n",
    "        file_path = os.path.join(c_path, file_name)\n",
    "        os.remove(file_path)\n",
    "    elif file_name.endswith(\"_combined.csv\"):\n",
    "        file_path = os.path.join(c_path, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Debugging print to check the contents of the dataframe\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        print(f\"DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {df.columns}\")\n",
    "\n",
    "        if 'Common label' not in df.columns:\n",
    "            print(f\"Skipping file: {file_name} because 'Common label' column is missing\")\n",
    "            continue\n",
    "\n",
    "        t_df = df.set_index(\"Common label\").T\n",
    "\n",
    "        animal_identifier = file_name.split(\"_\")[0]\n",
    "        transposed_dataframes[animal_identifier] = t_df\n",
    "\n",
    "# Debugging print to check the animal identifiers collected\n",
    "print(f\"Animal identifiers: {list(transposed_dataframes.keys())}\")\n",
    "\n",
    "combined_df = pd.concat(transposed_dataframes.values(), keys=transposed_dataframes.keys())\n",
    "# Save the combined dataframe\n",
    "output_file_path = os.path.join(c_output, \"results_abetaextra.csv\")\n",
    "combined_df.to_csv(output_file_path)\n",
    "\n",
    "print(\"Processing completed.\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MANOVA and PCA on whole brain data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"Singular matrix\" error in the context of MANOVA usually indicates that the matrix being used for the analysis is not of full rank, meaning it's not invertible or has linearly dependent columns. This can occur for several reasons. So perhaps, it is not possible to run MANOVA with this raw data. Consider removing some variables or using dimensionality reduction techniques, such as PCA and t-SNE.\n",
    "\n",
    "7. PCA: PCA is a linear dimensionality reduction technique that transforms the original variables into a new set of orthogonal variables called principal components. These components capture the maximum variance in the data. By selecting a subset of these principal components, you can effectively reduce the dimensionality of the data.\n",
    "8. Distributed Stochastic Neighbor Embedding (t-SNE): t-SNE is a nonlinear dimensionality reduction technique that aims to preserve pairwise distances between data points. It's often used for visualization and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('/Volumes/ANNA_HD/ANALYSIS/QUINT/output/tau/results_tau.csv', sep=';')\n",
    "column_titles = df.columns.tolist()\n",
    "output_list = column_titles[3:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df.drop(['animal', 'age'],axis=1))\n",
    "\n",
    "pca = PCA(n_components=None)\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting the explained variance ratio (also known as the scree plot), there are a few other visualizations you can consider to better understand your PCA results:\n",
    "\n",
    "1. Cumulative Explained Variance Plot: This plot shows the cumulative explained variance as the number of components increases. It can help you decide how many principal components to retain for your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Scatter Plots: You can create scatter plots of the first few principal components to visualize how data points are distributed based on these components. This can help you identify any patterns or clusters in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1 = pca_data[:, 0]\n",
    "pc2 = pca_data[:, 1]\n",
    "\n",
    "plt.scatter(pc1, pc2, c=df['age'], cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Scatter Plot of PC1 vs PC2')\n",
    "plt.colorbar(label='Group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('/Volumes/ANNA_HD/ANALYSIS/QUINT/output/tau/results_tau.csv', sep=';')\n",
    "df = df.loc[:, (df != 0).any()]\n",
    "column_titles = df.columns.tolist()\n",
    "output_list = column_titles[3:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df.drop(['animal', 'age'],axis=1))\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_.sum() *100\n",
    "figsize=(20, 20)\n",
    "pc1 = pca_data[:, 0]\n",
    "pc2 = pca_data[:, 1]\n",
    "pc3 = pca_data[:, 2]\n",
    "fig = px.scatter_3d(\n",
    "    x=pc1, y=pc2, z=pc3, color=df['age'], template=\"plotly_dark\")\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3'))\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"#29212B\"  # Set the custom background color\n",
    ")\n",
    "fig.update_layout(title='Tau')\n",
    "fig.update_layout(\n",
    "    scene_camera=dict(\n",
    "        up=dict(x=0, y=0, z=1),\n",
    "        center=dict(x=0, y=0, z=0),\n",
    "        eye=dict(x=2, y=2, z=2)\n",
    "    ),\n",
    "    scene_aspectmode='cube'  # Adjust the aspect ratio to make it closer to a cube\n",
    ")\n",
    "fig.show()\n",
    "# fig.write_image(\"/Users/annateruel/Desktop/3d_scatter_plot_int.svg\", scale=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MANOVA on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.multivariate.manova import MANOVA \n",
    "import pandas as pd\n",
    "df = pd.read_csv('/Volumes/ANNA_HD/ANALYSIS/QUINT/output/abeta_intra/results_abeta_intra.csv', sep=';')\n",
    "#df = df.loc[:, (df != 0).any()]\n",
    "column_titles = df.columns.tolist()\n",
    "output_list = column_titles[3:]\n",
    "\n",
    "formula = \" + \".join(output_list)\n",
    "full_formula = f\"{formula} ~ age\"\n",
    "print(full_formula)\n",
    "fit = MANOVA.from_formula(full_formula, data=df)\n",
    "print(fit.mv_test().summary_frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MANOVA from PC1, PC2, PC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.multivariate.manova import MANOVA \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = pd.read_csv('/Volumes/ANNA_HD/ANALYSIS/QUINT/output/abeta_intra/results_abeta_intra.csv', sep=';')\n",
    "column_titles = df.columns.tolist()\n",
    "output_list = column_titles[3:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df.drop(['animal', 'age'],axis=1))\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pc_scores = pd.DataFrame(pca.fit_transform(scaled_data), columns = ['PC1', 'PC2', 'PC3'])\n",
    "pc_scores['age'] = df['age']\n",
    "\n",
    "fit = MANOVA.from_formula('PC1 + PC2 + PC3~ age', data=pc_scores)\n",
    "print(fit.mv_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the PCA doesn't seem to group correctly by group of age, because there's huge variability, and it doesn't seem that abeta and tau reflect the age, we will run a clustergram to see if there are some brain areas more affected by the presence of beta and tau. \n",
    "\n",
    "First, to make data more readable, we're going to delete all columns where all values are 0. Thus, we will not consider brain regions where there is no pathological marker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Volumes/ANNA_HD/ANALYSIS/QUINT/output/abeta_intra/results_abeta_intra.csv', sep=';')\n",
    "df = df.loc[:, (df != 0).any()]\n",
    "df = df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see that from the 900 variables we used to have, we only have quantification in a total of 25 brain regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "corr_matrix = np.corrcoef(df.T).round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "column_names = df.columns\n",
    "\n",
    "plt.figure(figsize=(50, 50))  \n",
    "sns.heatmap(corr_matrix, annot = True, annot_kws={\"size\": 7})\n",
    "plt.xticks(np.arange(len(column_names)), column_names, rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(np.arange(len(column_names)), column_names, rotation=0, fontsize=8)  # Rotate y-axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "column_names = df.columns\n",
    "\n",
    "# Compute significance values for correlation matrix\n",
    "sig_corr_matrix = corr_matrix < 0.05  # Replace 0.05 with your desired significance threshold\n",
    "\n",
    "plt.figure(figsize=(50, 50))  \n",
    "sns.heatmap(corr_matrix, annot=sig_corr_matrix, fmt=\".2f\", cmap='coolwarm',\n",
    "            annot_kws={\"size\": 7})\n",
    "plt.xticks(np.arange(len(column_names)), column_names, rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(np.arange(len(column_names)), column_names, rotation=0, fontsize=8)  # Rotate y-axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MANOVA and PCA on amygdalo-entorhino-hippocampal pathway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing MANOVA. In our dataset we have several ages and theis associated object count per brain region in the amygdalo-entorhino-hippocampal pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.multivariate.manova import MANOVA \n",
    "import pandas as pd\n",
    "df = pd.read_csv('/Volumes/ANNA_HD/ANALYSIS/QUINT/output/abeta_extra/results_abeta_extra_2.csv', sep=';')\n",
    "#df = df.loc[:, (df != 0).any()]\n",
    "column_titles = df.columns.tolist()\n",
    "output_list = column_titles[3:]\n",
    "\n",
    "formula = \" + \".join(output_list)\n",
    "full_formula = f\"{formula} ~ age\"\n",
    "print(full_formula)\n",
    "fit = MANOVA.from_formula(full_formula, data=df)\n",
    "print(fit.mv_test().summary_frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for multicolineality in my dataframe we see there's strong collineality between several variables because they have values higher than 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "df = pd.read_csv('/Volumes/ANNA_HD/ANALYSIS/QUINT/output/tau/results_tau_2.csv', sep=';')\n",
    "df2 = df.drop(['animal', 'age'],axis=1)\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = df2.columns\n",
    "\n",
    "# Calculate VIF for each variable\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(df2.values, i) for i in range(df2.shape[1])]\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('/Volumes/ANNA_HD/ANALYSIS/QUINT/output/abeta_extra/results_abeta_extra_2.csv', sep=';')\n",
    "column_titles = df.columns.tolist()\n",
    "output_list = column_titles[3:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df.drop(['animal', 'age'],axis=1))\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_.sum() *100\n",
    "explained_variance_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = pd.read_csv('/Volumes/ANNA_HD/ANALYSIS/QUINT/output/tau/results_tau_2.csv', sep=';')\n",
    "df2 = df.drop(['animal', 'age'],axis=1)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(df2)\n",
    "\n",
    "pc1 = pca_data[:, 0]\n",
    "pc2 = pca_data[:, 1]\n",
    "\n",
    "plt.style.use('dark_background')  \n",
    "plt.scatter(pc1, pc2, c=df['age'], cmap='plasma', s=150, edgecolor='white', linewidth=1)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Scatter Plot of PC1 vs PC2')\n",
    "plt.colorbar(label='Group')\n",
    "plt.savefig('/Users/annateruel/Desktop/tau.svg', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biplot \n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "PC1 = pca.fit_transform(df2)[:,0]\n",
    "PC2 = pca.fit_transform(df2)[:,1]\n",
    "ldngs = pca.components_\n",
    "\n",
    "scalePC1 = 1.0/(PC1.max() - PC1.min())\n",
    "scalePC2 = 1.0/(PC2.max() - PC2.min())\n",
    "features = df2.feature_names\n",
    "\n",
    "plt.figure(figsize=(14,9))\n",
    " \n",
    "for i, feature in enumerate(features):\n",
    "    plt.arrow(0, 0, ldngs[0, i], \n",
    "             ldngs[1, i], \n",
    "              head_width=0.03, \n",
    "             head_length=0.03)\n",
    "    plt.text(ldngs[0, i] * 1.15, \n",
    "            ldngs[1, i] * 1.15, \n",
    "            feature, fontsize=18)\n",
    " \n",
    "sns.scatterplot(x=PC1 * scalePC1,\n",
    "                y=PC2 * scalePC2, \n",
    "                hue=target_groups,\n",
    "                palette=\"viridis\")\n",
    " \n",
    "plt.xlabel('PC1', fontsize=20)\n",
    "plt.ylabel('PC2', fontsize=20)\n",
    "plt.title('Figure 6', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pc_scores = pd.DataFrame(pca.fit_transform(df2), columns = ['PC1', 'PC2'])\n",
    "pc_scores['age'] = df['age']\n",
    "\n",
    "fit = MANOVA.from_formula('PC1 + PC2~ age', data=pc_scores)\n",
    "print(fit.mv_test())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda\n",
    "\n",
    "X = pc_scores[[\"PC1\", \"PC2\"]]\n",
    "y = pc_scores[\"age\"]\n",
    "post_hoc = lda().fit(X=X, y=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing correlation matrix on small dataset of the amygdalo-entorhino-hippocampal pathway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('/Volumes/ANNA_HD/ANALYSIS/QUINT/output/abeta_intra/results_abeta_intra_2.csv', sep=';')\n",
    "df = df.iloc[:, 1:]\n",
    "df = df.loc[:, (df != 0).any()]\n",
    "column_names = df.columns\n",
    "corr_matrix = np.corrcoef(df.T).round(decimals=2)\n",
    "\n",
    "plt.figure(figsize=(10, 10))  \n",
    "sns.heatmap(corr_matrix, annot = True, annot_kws={\"size\": 7})\n",
    "plt.xticks(np.arange(len(column_names)), column_names, rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(np.arange(len(column_names)), column_names, rotation=0, fontsize=8)  # Rotate y-axis labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.multivariate.manova import MANOVA \n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pc_scores = pd.DataFrame(pca.fit_transform(scaled_data), columns = ['PC1', 'PC2', 'PC3'])\n",
    "pc_scores['age'] = df['age']\n",
    "\n",
    "fit = MANOVA.from_formula('PC1 + PC2 + PC3~ age', data=pc_scores)\n",
    "print(fit.mv_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA on the amygdalo-entorhino-hippocampal pathway\n",
    "Evaluate results for structures in the amygdalo-entorhinal-hippocampal circuit independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_intra = pd.read_csv('/Volumes/ANNA_HD/ANALYSIS/QUINT/output/abeta_intra/results_abeta_intra_2.csv', sep=';')\n",
    "ab_extra = pd.read_csv('/Volumes/ANNA_HD/ANALYSIS/QUINT/output/abeta_extra/results_abeta_extra_2.csv', sep=';')\n",
    "tau = pd.read_csv('/Volumes/ANNA_HD/ANALYSIS/QUINT/output/tau/results_tau_2.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame to long format\n",
    "df2 = pd.melt(tau, id_vars=['animal', 'age'], var_name='brain_area', value_name='cells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Create the plot using seaborn with \"plasma\" colormap\n",
    "plt.style.use('dark_background')\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "plt.style.use(\"dark_background\")\n",
    "# g = sns.catplot(\n",
    "#     data=df2,\n",
    "#     x='brain_area',\n",
    "#     y='cells',\n",
    "#     hue='age',\n",
    "#     palette='plasma',\n",
    "#     s=100,\n",
    "#     alpha=0.7,\n",
    "#     kind='swarm',  # Use 'swarm' for separate points\n",
    "#     aspect=2  # Adjust this aspect value for width\n",
    "# )\n",
    "# g.set_xticklabels(rotation=45, horizontalalignment='right')\n",
    "sns.barplot(\n",
    "    data=df2,\n",
    "    x='brain_area',\n",
    "    y='cells',\n",
    "    hue='age',\n",
    "    palette='plasma',\n",
    "    errcolor='white',  # Set the color of error bars to white\n",
    "    errwidth=1.0,     # Set the linewidth of error bars to 1.0 (thinner)\n",
    "    errorbar= 'se'\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Brain Area')\n",
    "plt.ylabel('Cells')\n",
    "plt.title('Scatterplot and Barplot by Age and Brain Area')\n",
    "plt.legend(title='Age')\n",
    "\n",
    "plt.savefig('/Users/annateruel/Desktop/plot_filename.svg', format='svg', bbox_inches='tight')\n",
    "plt.savefig('/Users/annateruel/Desktop/plot_filename.svg', format='svg', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "plt.figure(figsize=(25, 10))\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "plt.style.use(\"dark_background\")\n",
    "g = sns.catplot(\n",
    "    data=df2,\n",
    "    x='brain_area',\n",
    "    y='cells',\n",
    "    hue='age',\n",
    "    palette='plasma',\n",
    "    kind='violin',\n",
    "    linewidth=0.5,\n",
    "    aspect=2  # Adjust this aspect value for width\n",
    ")\n",
    "g.set_xticklabels(rotation=45, horizontalalignment='right')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform linear regression\n",
    "formula = 'cells ~ age * brain_area'\n",
    "model = smf.ols(formula=formula, data=df2).fit()\n",
    "\n",
    "# Print ANOVA table\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "df2['age'] = df2['age'].astype(str)\n",
    "\n",
    "# Combine 'brain_area' and 'age' into a single interaction factor\n",
    "df2['interaction'] = df2['brain_area'] + '_' + df2['age']\n",
    "\n",
    "# Perform Tukey's HSD for pairwise comparisons of the interaction\n",
    "tukey_results = pairwise_tukeyhsd(df2['cells'], df2['brain_area'])\n",
    "tukey_df = pd.DataFrame(data=tukey_results._results_table.data[1:], columns=tukey_results._results_table.data[0])\n",
    "\n",
    "sorted_results = tukey_df.sort_values(by=['p-adj'], ascending=True)\n",
    "sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tukey_results = pairwise_tukeyhsd(df2['cells'], df2['interaction'], alpha=0.05)\n",
    "p_values_df = pd.DataFrame({'Comparison': tukey_results.groupsunique,\n",
    "                            'P-Value': tukey_results.pvalues})\n",
    "\n",
    "# Sort the DataFrame by p-values\n",
    "p_values_df = p_values_df.sort_values(by='P-Value')\n",
    "# Create a pivot table for the p-values DataFrame for the heatmap\n",
    "p_values_pivot = p_values_df.pivot(index='Comparison', columns='Comparison', values='P-Value')\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(p_values_pivot, annot=True, fmt='.4f', cmap='coolwarm', cbar=True)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('P-Values from Multiple Comparisons')\n",
    "plt.xlabel('Comparison')\n",
    "plt.ylabel('Comparison')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
